<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mark Salama</title>
    <description>Data Science Portfolio.</description>
    <link>http://www.marksalama.com/</link>
    <atom:link href="http://www.marksalama.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>2019-01-09</pubDate>
    <lastBuildDate>Wed, 09 Jan 2019 17:37:27 +0100</lastBuildDate>
    <generator>Jekyll v3.7.4</generator>
    
      <item>
        <title>Visualizing decision trees using ELI5</title>
        <description>&lt;p&gt;Decision trees, specifically &lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_boosting&quot;&gt;gradient boosted methods&lt;/a&gt;, are a powerful and easy-to-use machine learning technique for making predictions. They work by leveraging a large number of simple decision trees. For example, if we are trying to predict how students will score on an upcoming test, one tree might look like this.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/img/sample_decision_tree.png&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each tree individually doesn’t tell us much, but if we average the responses of hundreds of trees, the predictions become robust. I liken it to guessing how many candies are in a jar. One guess is random, but the average of 1,000 guesses is usually a good approximation.&lt;/p&gt;

&lt;p&gt;An issue with decision tree approaches is that it’s not immediately clear what relationships the model is learning. One can easily investigate one tree (XGBoost has a plot_tree method) but one tree doesn’t tell us much. Which brings us to the awesome &lt;a href=&quot;https://eli5.readthedocs.io/en/latest/index.html&quot;&gt;ELI5 package&lt;/a&gt;. ELI5, an acronym for explain like I’m five, offers ways to visualize and understand how your model is making predictions. Specifically, for a given observation, it can tell you the impact of each feature on the prediction. It works by looking at how the prediction changes across all of the trees based on a specific feature. Let’s consider one student who studied for 5 hours. ELI5 might tell us that across all of the trees, having a value of 5 for hours studied has a +10 impact on the predicted score for the upcoming test. When we extend this to a large number of observations, we can understand the relationships the model is learning.&lt;/p&gt;

&lt;p&gt;Using a dataset on hospital readmission, I want to walk through an example. The dataset is described in detail &lt;a href=&quot;https://www.hindawi.com/journals/bmri/2014/781670/&quot;&gt;here&lt;/a&gt; but essentially there are features pertaining to a hospital visit such as the number of diagnoses made, length of visit, and how many procedures were performed, and the target variable is if they were readmitted to the hospital. I trained an XGBoost model, the code is on my &lt;a href=&quot;https://github.com/mark-salama/eli_hospital_readmissions&quot;&gt;GitHub&lt;/a&gt;, and built this graphic to visualize the relationship the model learned between the number of diagnoses entered into the system during the patient’s visit and the probability of being readmitted.
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/img/diagnoses.png&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
Note that a positive weight is related to a higher predicted probability of readmission. We can see that up until 5 diagnoses, this feature has a relatively consistent negative weight, so it contributes to a lower chance of being readmitted. But if an observation has more than 6 diagnoses, this feature has a positive weight, so a higher chance of readmission. And just as the difference between 3,4 and 5 diagnoses on the low end is muted, there is little difference between 7, 8 or 9 diagnoses. Intuitively this makes sense. Decision trees are great for learning non-linear relationships, and with this method, we can visualize what the model learned!&lt;/p&gt;
</description>
        <pubDate>2019-01-09</pubDate>
        <link>http://www.marksalama.com/articles/2019-01/visualizing-decision-trees</link>
        <guid isPermaLink="true">http://www.marksalama.com/articles/2019-01/visualizing-decision-trees</guid>
        
        
        <category>visualization</category>
        
        <category>prediction</category>
        
      </item>
    
      <item>
        <title>Visualizing Brooklyn Home Price Appreciation</title>
        <description>&lt;p&gt;The last post was a GIF showing annual home price appreciation by neighborhood for Manhattan. Here it is for Brooklyn.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/img/brooklyn_appreciation.gif&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Brooklyn has seen greater appreciation than Manhattan, especially in the last few years.&lt;/p&gt;
</description>
        <pubDate>2019-01-09</pubDate>
        <link>http://www.marksalama.com/articles/2019-01/visualizing-brooklyn-home-price-appreciation</link>
        <guid isPermaLink="true">http://www.marksalama.com/articles/2019-01/visualizing-brooklyn-home-price-appreciation</guid>
        
        
        <category>visualization</category>
        
        <category>markets</category>
        
      </item>
    
      <item>
        <title>Visualizing Manhattan Home Price Appreciation</title>
        <description>&lt;p&gt;NYC makes &lt;a href=&quot;https://www1.nyc.gov/site/finance/taxes/property-annualized-sales-update.page&quot;&gt;annual home sales data&lt;/a&gt; available going back to 2003. They also provide &lt;a href=&quot;https://www1.nyc.gov/site/planning/data-maps/open-data/dwn-nynta.page&quot;&gt;geographic data&lt;/a&gt; that breaks NYC into neighborhoods. To better understand home appreciation by neighborhood, I combined the datasets and built this GIF.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/img/manhattan_appreciation.gif&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The only tricky part was figuring out how to associate each transaction with a neighborhood. I found the latitude and longitude of all zip codes for which there was a transaction, and then used geopandas to join each zip code to a neighborhood. It would have been more accurate to find the latitude and longitude for each address but that would have required a large number of Google API calls.&lt;/p&gt;

</description>
        <pubDate>2019-01-09</pubDate>
        <link>http://www.marksalama.com/articles/2019-01/visualizing-manhattan-home-price-appreciation</link>
        <guid isPermaLink="true">http://www.marksalama.com/articles/2019-01/visualizing-manhattan-home-price-appreciation</guid>
        
        
        <category>visualization</category>
        
        <category>markets</category>
        
      </item>
    
      <item>
        <title>Sell in May and (Facebook) Prophet</title>
        <description>&lt;p&gt;Facebook released a time series analysis package called &lt;a href=&quot;https://facebook.github.io/prophet/docs/quick_start.html&quot;&gt;Prophet&lt;/a&gt; which makes it easy to work with time series data. I loaded up S&amp;amp;P 500 data, and when looking at the seasonal decomposition using the plot_components method, I noticed that there was a peak around May. It explained the old saying &lt;a href=&quot;https://en.wikipedia.org/wiki/Sell_in_May&quot;&gt;‘sell in may and go away’&lt;/a&gt;. The below gif shows the seasonal decomposition for the trailing 20 year period.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/img/fb_may.gif&quot; alt=&quot;alt text&quot; title=&quot;Sell in May&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>2019-01-09</pubDate>
        <link>http://www.marksalama.com/articles/2018-12/sell-in-may</link>
        <guid isPermaLink="true">http://www.marksalama.com/articles/2018-12/sell-in-may</guid>
        
        
        <category>visualization</category>
        
        <category>markets</category>
        
      </item>
    
      <item>
        <title>Find Your Podcast</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://www.findyourpodcast.com&quot;&gt;&lt;strong&gt;Find Your Podcast&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Continuing with recommender systems, I wanted to build something for podcasts. I’ve found new podcasts by looking through the charts on iTunes but I prefer a personalized solution. I follow a few podcasts on twitter and I have had success with association analysis in the past, so I wondered if I could identify similar podcasts by looking at overlapping twitter followers. To test the concept, I downloaded the follower lists for two similar podcasts and found that approximately 50% of the followers were overlapping.&lt;/p&gt;

&lt;p&gt;Using the podcasts database from &lt;a href=&quot;https://www.listennotes.com&quot;&gt;Listen Notes&lt;/a&gt; and the Twitter API, I extended this concept to 1,000 podcasts and built a website using Django. I received feedback that people weren’t finding their favorite podcasts which led me to extend it to 3,000 podcasts. I received the same feedback but without a paid Twitter Developer account, one can only download a certain number of follower lists at one time, which makes it hard to scale to additional podcasts, or keep recommendations up-to-date. So you might not find your favorite podcast, but the recommendations work pretty well.&lt;/p&gt;
</description>
        <pubDate>2019-01-09</pubDate>
        <link>http://www.marksalama.com/articles/2018-10/find-your-podcast</link>
        <guid isPermaLink="true">http://www.marksalama.com/articles/2018-10/find-your-podcast</guid>
        
        
        <category>recommender systems</category>
        
        <category>podcasts</category>
        
      </item>
    
      <item>
        <title>Movie Recommender Website</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://rs-squared.herokuapp.com&quot;&gt;&lt;strong&gt;Find Movies You’ll Love&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Since I learned about matrix factorization techniques I have been excited by the power of recommender systems. It’s amazing that ratings data alone can illuminate which movies are similar, yet options for exploring similar movies based on a given movie are limited. Basically recommender systems are black boxes. Moreover, what happens when you want to watch a movie with a friend or partner? Netflix doesn’t have a way to explore recommendations based on a starting movie you both agree on. With these concerns in mind, I built a simple recommender system using Dash. Please note that it’s slow to load because I’m using a free version of Heroku.&lt;/p&gt;

&lt;p&gt;To arrive at similarities for each movie, I used the &lt;a href=&quot;http://surpriselib.com&quot;&gt;surprise&lt;/a&gt; python package to build a similarity matrix capturing how similar each movie is to every other movie. As this matrix is very large (with n^2 entries) I employed principal component analysis (PCA) with 10 components to create a more manageable dataset. I found that 10 components was a good balance of complexity and predictive power. When a user selects a movie from the dropdown, movies with the most similar vector representations (from the PCA of the similarity matrix) are displayed. As much of the impetus for this project was to be able to identify recommendations based on multiple starting movies, I thought a lot about how to go two or more movies, to a list of similar movies, but ultimately settled on simply averaging the vectors of the selected movies.&lt;/p&gt;
</description>
        <pubDate>2019-01-09</pubDate>
        <link>http://www.marksalama.com/articles/2018-08/movie-recommender</link>
        <guid isPermaLink="true">http://www.marksalama.com/articles/2018-08/movie-recommender</guid>
        
        
        <category>recommender systems</category>
        
        <category>movies</category>
        
      </item>
    
      <item>
        <title>Movie Similarities using Matrix Factorization</title>
        <description>&lt;p&gt;For an assignment in a machine learning class, we were given a file with 95,000 movie ratings in the form of - user_id, movie_id and rating. From this we can build what is called an incomplete ratings matrix and the assignment was to fill in the matrix using probabilistic matrix factorization which is a form of &lt;a href=&quot;https://en.wikipedia.org/wiki/Collaborative_filtering&quot;&gt;collaborative filtering&lt;/a&gt;. There are 1,682 different movies and 943 different users in the data. Since we have 95,000 ratings, there are 1.5 million incomplete (missing) ratings or 94%. Here is an example of a ratings matrix. In practice, there are more blanks.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/img/example_matrix.png&quot; alt=&quot;alt text&quot; title=&quot;Example Matrix&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To perform matrix factorization, we decide on a number of features (e.g., 10), and factorize the ratings matrix into two matrices, one for the movies and one for the users, each with 10 dimensions (the number of features we settled on). Each row of the movie matrix corresponds to one movie and we learn 10 numbers for each movie, a movie vector. Similarly, each row of the user matrix corresponds to one user and we learn 10 numbers for each user, a user vector. It is an iterative process where we first update each user vector based on each movie they have rated, and the rating they gave it and then update each movie vector based on the information we have about each user who rated this movie (which is contained in the user vector) and the rating that user gave. We repeat this process until the changes in the movie and user vectors drop below some threshold. The code is at the bottom of this post and on my GitHub page. The idea is that once we have learned the two matrices, we can predict a missing value in the ratings matrix using the respective movie and user vectors. For example, if we want to predict how Erica would rate Pulp Fiction, we would multiply the row of the user matrix corresponding to Erica by the row of the movie matrix corresponding to Pulp Fiction.&lt;/p&gt;

&lt;p&gt;A nice way to test whether the 10-numbers (features) that we have learned have any meaning, is to see if similar movies have similar values for the 10 features. To do so, we select a movie, and then calculate its distance to every other movie (distance is essentially the sum of the respective differences in the 10 numbers). Here are the 10 movies closest to ‘Star Wars’, ‘Lion King’ and ‘Dumb and Dumber’.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/img/similar_movies_table.png&quot; alt=&quot;alt text&quot; title=&quot;Similar Movies Table&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we were able to visualize a 10-dimensional space, these would be the ten movies that are closest to the given movie in that space. To view it in 2D, we can use a feature of scikit-learn called t-SNE to perform dimensionality reduction and reduce each movie to 2-dimensions. As this resulted in many points that were extremely close to each other, I used the ggrepel library in R to separate the data points.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/img/similar_movies_plot.png&quot; alt=&quot;alt text&quot; title=&quot;Similar Movies Plot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The three Star Wars movies are right next to each other, same with The Rock, Independence Day and Air Force One, but what is Liar Liar doing so close by? It’s nice to be able to compare the movies in a scatterplot but we lost information.&lt;/p&gt;

&lt;p&gt;Back to 10-dimensions, I was curious if I could understand what the 10 factors that the model learns actually capture. To investigate this, I looked at what movies had the highest and lowest values for each of the ten factors. When I did this the first time, I didn’t recognize most of the movies, so I only considered the 250 movies with the highest number of ratings. I had trouble deciphering many of the factors but found these two to be clear.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/img/movie_factors.png&quot; alt=&quot;alt text&quot; title=&quot;Factors&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The factor on the left captures dark vs. romantic with crime dramas high in the factor and romantic dramas low in the factor. The right factor appears to be musical/comedy vs. action. Pretty amazing that this information can be extracted from 95,000 movie ratings. Netflix has a lot more than that.&lt;/p&gt;

&lt;p&gt;The ratings data was adapted from the GroupLens Research Project at the University of Minnesota by &lt;a href=&quot;http://www.columbia.edu/~jwp2128/&quot;&gt;John Paisley&lt;/a&gt;, the professor of my Machine Learning course. I used Python to iteratively build the movie and user matrices and ggplot to plot the movie locations.&lt;/p&gt;

&lt;!-- Here is the code for probabilistic matrix factorization. We initialize each movie and user to a random set of 10 numbers and then update these arrays through matrix operations.

```python
def matrix_fact(num_fact, interations):
    user_list = []
    movie_list = []
    for i in range(interations): 
    
        user_m = np.random.multivariate_normal(np.zeros(num_fact),np.eye(num_fact),943)
        movie_m = np.random.multivariate_normal(np.zeros(num_fact),np.eye(num_fact),1682)
        first_part = lamb*varsq*np.eye(num_fact)

        for t in range(100):

            for i in range(user_m.shape[0]):

                locas_nan = np.where(~np.isnan(matrix_m[i,:]))
                v_dot = np.dot(np.transpose(movie_m[locas_nan]),movie_m[locas_nan])
                m_v_dot = np.dot(matrix_m[i][locas_nan],movie_m[locas_nan])         
                user_m[i] = (np.dot(np.linalg.inv(first_part+v_dot),m_v_dot)).flatten()

            for k in range(movie_m.shape[0]):

                locas_nan = np.where(~np.isnan(matrix_m[:,k]))
                u_dot = np.dot(np.transpose(user_m[locas_nan]),user_m[locas_nan])
                m_u_dot = np.dot(matrix_m[:,k][locas_nan],user_m[locas_nan])
                movie_m[k] = (np.dot(np.linalg.inv(first_part+u_dot),m_u_dot)).flatten()
        
        user_list.append(user_m)
        movie_list.append(movie_m)
        
    return user_list, movie_list
``` --&gt;
</description>
        <pubDate>2019-01-09</pubDate>
        <link>http://www.marksalama.com/articles/2017-05/movie-pmf</link>
        <guid isPermaLink="true">http://www.marksalama.com/articles/2017-05/movie-pmf</guid>
        
        
        <category>recommender systems</category>
        
        <category>movies</category>
        
      </item>
    
      <item>
        <title>NYC Neighborhood Average Age through Citibike Usage</title>
        <description>&lt;p&gt;Citibike makes data available for all rides on their network. We have the starting station location of each trip, and for most trips, the birth year of the rider. I approximated age from the birth year variable, and using the Google API, I associated each Citibike station with a neighborhood in NYC (e.g., Chelsea or Williamsburg). I then looked at the average age of riders by starting neighborhood.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/img/citibike_age_neigh.png&quot; alt=&quot;alt text&quot; title=&quot;Age Neighborhood&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The most common age (mode) is actually the same for UWS and Williamsburg - 31 years old. The distributions below show why the averages are so different.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/age_distributions.png&quot; alt=&quot;alt text&quot; title=&quot;Age Neighborhood&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I used a combination of pandas, R (ggplot), and seaborn to produce these graphics.&lt;/p&gt;
</description>
        <pubDate>2019-01-09</pubDate>
        <link>http://www.marksalama.com/articles/2017-05/bike-share-2</link>
        <guid isPermaLink="true">http://www.marksalama.com/articles/2017-05/bike-share-2</guid>
        
        
        <category>visualization</category>
        
        <category>biking</category>
        
      </item>
    
      <item>
        <title>Citibike Heatmaps 2016</title>
        <description>&lt;p&gt;Here is a heatmap of the approx. 13.8 million Citibike rides in 2016, with each day as a column and every 15 minutes as a row.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/img/citibike_full_2016.png&quot; alt=&quot;alt text&quot; title=&quot;Citibike Heatmap 2016&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We immediately see the distinction between working days when people use citibikes to get to and from work and weekends and holidays when riding primarily occurs during the day. It is also clear that riding peaks in September and October which can be explained by the great fall weather in New York. To focus on commuting behavior, I removed all weekends and holidays.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/img/citibike_working_2016.png&quot; alt=&quot;alt text&quot; title=&quot;Citibike Heatmap 2016&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Looking at rides on working days we see that commutes home truly start at 5pm, there is a clear distinction moving from the 4:45 - 5:00 increment to the 5:00 - 5:15 increment. Similarly, there is a drop off from the 8:45 - 9:00 increment to the 9:00 - 9:15 increment. There is also a relatively dark area between 10:45 and 11:30 which I believe captures the lull between commutes to work and the lunch rush.
&lt;br /&gt;
&lt;br /&gt;
I used pandas to combine the quarterly CSVs made available by Citibike, and then exported a group by function to R where I used ggplot to produce these heatmaps.&lt;/p&gt;
</description>
        <pubDate>2019-01-09</pubDate>
        <link>http://www.marksalama.com/articles/2017-05/bike-share-1</link>
        <guid isPermaLink="true">http://www.marksalama.com/articles/2017-05/bike-share-1</guid>
        
        
        <category>visualization</category>
        
        <category>biking</category>
        
      </item>
    
  </channel>
</rss>
